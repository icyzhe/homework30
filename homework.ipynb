{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISC7401 Homework 3 \n",
    "## Deep Learning for Image Recognition \n",
    "@Author: Zhe He\n",
    "\n",
    "Overall task: Given a training set of data, you should train a “deep model” (e.g., multi-layer\n",
    "perceptron, convolutional neural network, and transformer), evaluate your model on the\n",
    "testing set and some open-set data w.r.t some metrics, and write up a report.\n",
    "\n",
    "Train different deep neural networks (including MLP, ResNet, ViT transformer) on the training\n",
    "set. [This website and links therein](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) might be important to you.\n",
    "You can refer to PyTorch for [building your network](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html), or using other established networks such as\n",
    "ResNet and Transformer. When training the deep neural networks for classification, you can use\n",
    "cross-entropy loss.\n",
    "\n",
    "See homework requirement for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare phase\n",
    "### First, before writing training code, let's define some helper function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "from torch.utils.data import Dataset,ConcatDataset,DataLoader\n",
    "from torch import nn, optim\n",
    "import torch \n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torchvision import transforms\n",
    "import timm \n",
    "import timm.optim\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTdataset(Dataset):\n",
    "    def __init__(self, data, transform = None):\n",
    "        self.X = (data.drop('label', axis =1).values/255).reshape((-1, 28, 28,1))\n",
    "        self.y = data['label'].values.reshape(-1,1)\n",
    "        self.n_samples = data.shape[0]\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X,y = self.X[index], self.y[index]\n",
    "        if self.transform:\n",
    "            X  = self.transform(X)\n",
    "        return (X,y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class MNISTdataset_inference(Dataset):\n",
    "    \"\"\"Inference\"\"\"\n",
    "    def __init__(self, data, transform = None):\n",
    "        self.X = (data.values/255).reshape((-1, 28, 28,1))\n",
    "        self.n_samples = data.shape[0]\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        if self.transform:\n",
    "            X  = self.transform(X)\n",
    "        return X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Validation sets    \n",
    "train_df = pd.read_csv('dataset/train.csv',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "We use a ResNet50 as our transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class confiFileResNet():\n",
    "    \"\"\"Configuration class for easy parametrization\"\"\"\n",
    "    \n",
    "    #Pretrained model with timm\n",
    "    model = 'resnet50'\n",
    "    epochs = 30\n",
    "    \n",
    "    in_chans = 1\n",
    "    num_classes = 10\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    val_size = 0.3\n",
    "    batch_size = 128\n",
    "    \n",
    "CFG_ResNet = confiFileResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_transforms = {\n",
    "    'original': transforms.Compose([transforms.ToTensor()]),\n",
    "    'aug1': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=(300, 350), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(28, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug2': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=(0, 45), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(28, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug3': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=(-15, 15)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(28, scale=(0.7, 1.3))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def train_(model, optmizer, loss_func, train_loader, device):\n",
    "    \"\"\"Function to train the model\"\"\"\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    size_sampler = len(train_loader.sampler)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Pushing to device (cuda or CPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        #zeroing gradiants\n",
    "        optmizer.zero_grad()\n",
    "        \n",
    "        #feedfoard\n",
    "        y_hat = model(images)\n",
    "        \n",
    "        #Compute loss \n",
    "        loss = loss_func(y_hat, labels.long().squeeze())\n",
    "        \n",
    "        #Compute backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #updating weights\n",
    "        optmizer.step()\n",
    "        \n",
    "        # loss and correct values compute\n",
    "        train_loss +=loss.item() * images.size(0)\n",
    "        _ , pred = torch.max(y_hat.data, 1)\n",
    "        train_correct +=sum(pred == labels.long().squeeze()).sum().item()\n",
    "        \n",
    "    return np.round(train_loss/size_sampler,4), np.round(train_correct*100./size_sampler,3)\n",
    "\n",
    "\n",
    "def validation_(model, loss_func, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            y_hat = model(images)\n",
    "            loss = loss_func(y_hat, labels.long().squeeze())\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, preds = torch.max(y_hat, dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.squeeze())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.sampler)\n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100  # 百分比形式，与原代码保持一致\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'val_loss': np.round(val_loss, 4),\n",
    "        'accuracy': np.round(accuracy, 3),\n",
    "        'precision': np.round(precision, 4),\n",
    "        'recall': np.round(recall, 4),\n",
    "        'f1': np.round(f1, 4),\n",
    "        'conf_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, optmizer, loss_func, scheduler, train_loader, val_loader, epochs, device, log=True):\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    print('Initializing Training...')\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc = train_(model, optmizer, loss_func, train_loader, device)\n",
    "        val_metrics = validation_(model, loss_func, val_loader, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        overfitting = train_acc - val_acc\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            print(f'>> Saving Best Model with Val Acc: Old: {best_acc} -> New: {val_acc}')\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_acc = val_acc\n",
    "            best_metrics = val_metrics\n",
    "            best_metrics['overfitting'] = overfitting\n",
    "        \n",
    "        if log and ((i + 1) % 2 == 0):\n",
    "            print(f'> Epochs: {i+1}/{epochs} - Train Loss: {train_loss} - Train Acc: {train_acc} - '\n",
    "                  f'Val Loss: {val_metrics[\"val_loss\"]} - Val Acc: {val_acc} - '\n",
    "                  f'Precision: {val_metrics[\"precision\"]} - Recall: {val_metrics[\"recall\"]} - '\n",
    "                  f'F1: {val_metrics[\"f1\"]}')\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_metrics['val_loss'])\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "    \n",
    "    print('Training Finished:)')\n",
    "    return history, best_model, best_metrics\n",
    "\n",
    "def get_model(model_type):\n",
    "    if model_type == 'MLP':\n",
    "        return timm.create_model('mixer_b16_224', pretrained=True, in_chans=3, num_classes=10).to(device)\n",
    "    elif model_type == 'ResNet':\n",
    "        return timm.create_model('resnet50', pretrained=True, in_chans=1, num_classes=10).to(device)\n",
    "    elif model_type == 'ViT':\n",
    "        return timm.create_model('vit_base_patch16_224', pretrained=True, in_chans=3, num_classes=10).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "\n",
    "def plot_history(history):\n",
    "    fig1, ax1 = plt.subplots(figsize=(8,6))\n",
    "    sns.lineplot(data=history['train_loss'], label='Training Loss', ax=ax1)\n",
    "    sns.lineplot(data=history['val_loss'], label='Validation Loss', ax=ax1)\n",
    "    ax1.tick_params(axis='both', labelsize=32)\n",
    "    ax1.set_xlabel('Epoch', fontsize=32)\n",
    "    ax1.set_ylabel('Loss', fontsize=32)\n",
    "    ax1.legend(loc='upper right', fontsize=32)\n",
    "    fig1.savefig('loss_curve.png')\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(8,6))\n",
    "    sns.lineplot(data=history['train_acc'], label='Training Accuracy', ax=ax2)\n",
    "    sns.lineplot(data=history['val_acc'], label='Validation Accuracy', ax=ax2)\n",
    "    ax2.tick_params(axis='both', labelsize=32)\n",
    "    ax2.set_xlabel('Epoch', fontsize=32)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=32)\n",
    "    ax2.legend(loc='lower right', fontsize=32)\n",
    "    fig2.savefig('accuracy_curve.png')\n",
    "    plt.close(fig2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer\n",
    "We use a pre trained ViT to transfer learning in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class confiFileViT():\n",
    "    model = 'vit_base_patch16_224'\n",
    "    epochs = 30\n",
    "    in_chans = 3\n",
    "    num_classes = 10\n",
    "    learning_rate = 3e-5\n",
    "    val_size = 0.3\n",
    "    batch_size = 64\n",
    "CFG_ViT = confiFileViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_transform_base = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vit_transforms = {\n",
    "    'original': vit_transform_base,\n",
    "    'aug1': transforms.Compose([\n",
    "        vit_transform_base,\n",
    "        transforms.RandomRotation(degrees=(300, 350), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug2': transforms.Compose([\n",
    "        vit_transform_base,\n",
    "        transforms.RandomRotation(degrees=(0, 45), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug3': transforms.Compose([\n",
    "        vit_transform_base,\n",
    "        transforms.RandomRotation(degrees=(-15, 15)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.3))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron\n",
    "We use a pre trained MLP to do transfer learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class confiFileMLP():\n",
    "    model = 'mixer_b16_224'  # Pretrained MLP-Mixer model from timm\n",
    "    pretrained_cfg = 'miil_in21k_ft_in1k'  # Pretrained configuration\n",
    "    epochs = 30\n",
    "    in_chans = 3            # MLP-Mixer requires 3-channel input\n",
    "    num_classes = 10        # MNIST has 10 classes\n",
    "    learning_rate = 5e-5    # Smaller learning rate suitable for MLP-Mixer\n",
    "    img_size = 224          # MLP-Mixer requires 224x224 input\n",
    "    batch_size = 256        # Batch size (adjust based on GPU memory)\n",
    "    val_size = 0.3          # Validation set size\n",
    "\n",
    "CFG_MLP = confiFileMLP()\n",
    "\n",
    "mlp_transform_base = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mlp_transforms = {\n",
    "    'original': mlp_transform_base,\n",
    "    'aug1': transforms.Compose([\n",
    "        mlp_transform_base,\n",
    "        transforms.RandomRotation(degrees=(300, 350), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug2': transforms.Compose([\n",
    "        mlp_transform_base,\n",
    "        transforms.RandomRotation(degrees=(0, 45), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
    "    ]),\n",
    "    'aug3': transforms.Compose([\n",
    "        mlp_transform_base,\n",
    "        transforms.RandomRotation(degrees=(-15, 15)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.3))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# mixer_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert 1-channel to 3-channel\n",
    "#     transforms.Resize((CFG_MLP.img_size, CFG_MLP.img_size)),  # Resize to 224x224\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "#                          std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# compose_aug_mlp = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "#     transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "#     transforms.RandomResizedCrop(CFG_MLP.img_size, scale=(0.85, 1.0)),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# compose_original = mixer_transform\n",
    "\n",
    "# dataset_original = MNISTdataset(train_df, transform=compose_original)\n",
    "# dataset_aug = MNISTdataset(train_df, transform=compose_aug_mlp)\n",
    "\n",
    "# increased_dataset = ConcatDataset([dataset_original, dataset_aug])\n",
    "\n",
    "# train_df = pd.read_csv('dataset/train.csv', dtype=np.float32)\n",
    "\n",
    "# X_train, X_val = train_test_split(increased_dataset, test_size=CFG_MLP.val_size, random_state=666)\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(X_train, batch_size=CFG_MLP.batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(X_val, batch_size=CFG_MLP.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv', dtype=np.float32)\n",
    "model_type = 'MLP'  # Change to 'MLP' or 'ViT' as needed\n",
    "aug_strategy = 'original'  # Change to 'original', 'aug2', or 'aug3' as needed\n",
    "\n",
    "# Select transforms based on model\n",
    "if model_type == 'ResNet':\n",
    "    transform = resnet_transforms[aug_strategy]\n",
    "    val_transform = resnet_transforms['original']  # Validation set typically unaugmented\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    lr = 1e-3\n",
    "elif model_type == 'ViT':\n",
    "    transform = vit_transforms[aug_strategy]\n",
    "    val_transform = vit_transforms['original']\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    lr = 3e-5\n",
    "elif model_type == 'MLP':\n",
    "    transform = mlp_transforms[aug_strategy]\n",
    "    val_transform = mlp_transforms['original']\n",
    "    batch_size = 256\n",
    "    epochs = 30\n",
    "    lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = MNISTdataset(train_df, transform=transform)\n",
    "val_dataset = MNISTdataset(train_df, transform=val_transform)\n",
    "train_set, val_set = train_test_split(train_dataset, test_size=0.3, random_state=666)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "model = get_model(model_type)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "history, best_model, best_metrics = train_model(\n",
    "    model, optimizer, loss_func, scheduler, train_loader, val_loader, epochs, device\n",
    ")\n",
    "\n",
    "results = {\n",
    "    'model': model_type,\n",
    "    'aug_strategy': aug_strategy,\n",
    "    'accuracy': best_metrics['accuracy'],\n",
    "    'overfitting': best_metrics['overfitting'],\n",
    "    'f1': best_metrics['f1'],\n",
    "    'precision': best_metrics['precision'],\n",
    "    'recall': best_metrics['recall']\n",
    "}\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(f'results_{model_type}_{aug_strategy}.csv', index=False)\n",
    "\n",
    "\n",
    "test_transforms = {\n",
    "    'ResNet': transforms.Compose([transforms.ToTensor()]),\n",
    "    'ViT': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'MLP': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "#Test Sets\n",
    "test_df = pd.read_csv('dataset/test.csv',dtype=np.float32)\n",
    "test_dataset = MNISTdataset_inference(test_df, transform = test_transforms[model_type])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X_test_ld in (test_loader):\n",
    "        y_pred = best_model(X_test_ld.to(device))\n",
    "        _, y_pred_tags = torch.max(y_pred.data, dim = 1)\n",
    "#         y_pred_list.append((y_pred_tags.to('cpu')))\n",
    "\n",
    "result = [int(i) for i in y_pred_list]\n",
    "df = pd.DataFrame({'ImageId': range(1, len(result) + 1), 'Label': result})\n",
    "output_filename = f'dataset/output_{model_type}_{aug_strategy}.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "sub_status = os.path.exists(output_filename)\n",
    "\n",
    "print(f\"\\Check for {model_type} with {aug_strategy}: {sub_status}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save model and clear memory\n",
    "torch.save(best_model.state_dict(), f'model_{model_type}_{aug_strategy}.pth')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Finished training {model_type} with {aug_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "result_files = [f for f in os.listdir() if f.startswith('results_')]\n",
    "all_results = pd.concat([pd.read_csv(f) for f in result_files], ignore_index=True)\n",
    "\n",
    "# Save combined results\n",
    "all_results.to_csv('ablation_study_results.csv', index=False)\n",
    "\n",
    "# Plot augmentation strategy vs. performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model in ['MLP', 'ResNet', 'ViT']:\n",
    "    subset = all_results[all_results['model'] == model]\n",
    "    plt.plot(subset['aug_strategy'], subset['accuracy'], marker='o', label=model)\n",
    "plt.xlabel('Augmentation Strategy')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Accuracy Across Augmentation Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('accuracy_vs_augmentation.png')\n",
    "plt.show()\n",
    "\n",
    "# Display results table\n",
    "print(all_results[['model', 'aug_strategy', 'accuracy', 'overfitting', 'f1', 'precision', 'recall']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
